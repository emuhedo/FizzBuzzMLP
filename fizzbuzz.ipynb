{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import utils\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fizzbuzz(x):\n",
    "#  | \"fizzbuzz\" if x % 15 == 0\n",
    "#  | \"fizz\" if x % 3 == 0\n",
    "#  | \"buzz\" if x % 5 == 0\n",
    "#  | str(x) otherwise\n",
    "\n",
    "# x := [0, 100]\n",
    "# y := [f(x)]\n",
    "\n",
    "def get_binary(number: int, nbits: int) -> List[int]:\n",
    "    \"\"\"Given a int, returns its little-endian notation.\"\"\"\n",
    "\n",
    "    return [number >> i & 1 for i in range(nbits)]\n",
    "\n",
    "\n",
    "def fizzbuzz(x):\n",
    "    \"\"\"The actual function that we are trying to learn.\"\"\"\n",
    "\n",
    "    if x % 15 == 0:\n",
    "        return 'fizzbuzz'\n",
    "    elif x % 3 == 0:\n",
    "        return 'fizz'\n",
    "    elif x % 5 == 0:\n",
    "        return 'buzz'\n",
    "    return ''\n",
    "\n",
    "\n",
    "def build_dataset(lower: int, upper: int, nbits: int) -> List[List[int]]:\n",
    "    \"\"\"Builds fizzbuzz labels in the interval [lower, upper).\"\"\"\n",
    "\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for x in range(lower, upper + 1):\n",
    "        X.append(get_binary(x, nbits))\n",
    "        y.append(fizzbuzz(x))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FizzBuzz(nn.Module):\n",
    "# architecture:\n",
    "#    input -> FC(ReLU) -> FC(ReLU) -> Softmax\n",
    "\n",
    "    def __init__(self, input_sz:int, h1: int, h2: int, output_sz: int) -> None:\n",
    "        super(FizzBuzz, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_sz, h1)\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.projection = nn.Linear(h2, output_sz)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.linear1(inputs))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 train loss: 26.0902 train accuracy 0.5413 valid. accuracy 0.4973\n",
      "Epoch 2000 train loss: 15.4321 train accuracy 0.7564 valid. accuracy 0.6541\n",
      "Epoch 3000 train loss: 3.1704 train accuracy 0.9689 valid. accuracy 0.8595\n",
      "Epoch 4000 train loss: 1.1123 train accuracy 0.9878 valid. accuracy 0.9135\n",
      "Epoch 5000 train loss: 0.4288 train accuracy 0.9986 valid. accuracy 0.9243\n",
      "Epoch 6000 train loss: 0.2127 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 7000 train loss: 0.1500 train accuracy 1.0000 valid. accuracy 0.9297\n",
      "Epoch 8000 train loss: 0.0924 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 9000 train loss: 0.0708 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 10000 train loss: 0.0551 train accuracy 1.0000 valid. accuracy 0.9297\n",
      "Epoch 11000 train loss: 0.0459 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 12000 train loss: 0.0381 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 13000 train loss: 0.0324 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 14000 train loss: 0.0285 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 15000 train loss: 0.0279 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 16000 train loss: 0.0228 train accuracy 1.0000 valid. accuracy 0.9297\n",
      "Epoch 17000 train loss: 0.0213 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 18000 train loss: 0.0182 train accuracy 1.0000 valid. accuracy 0.9243\n",
      "Epoch 19000 train loss: 0.0196 train accuracy 1.0000 valid. accuracy 0.9297\n",
      "Epoch 20000 train loss: 0.0175 train accuracy 1.0000 valid. accuracy 0.9297\n"
     ]
    }
   ],
   "source": [
    "ceil = lambda x: int(np.ceil(x))\n",
    "\n",
    "def trainer(model: nn.Module,\n",
    "            X: torch.Tensor,\n",
    "            y: torch.Tensor,\n",
    "            n_epochs: int,\n",
    "            batch_size: int,\n",
    "            validation_frac: float,\n",
    "            log_every: int\n",
    "           ) -> None:\n",
    "    \"\"\"Helper function to train the model.\n",
    "    :param model: The PyTorch model\n",
    "    :param X: Tensors representing the sample features\n",
    "    :param y: Tensors with categorical labels\n",
    "    :param n_epochs\n",
    "    :param batch_size\n",
    "    :param validation_frac: Percentage of the samples used for validation\n",
    "    only after shuffling\n",
    "    :param log_every: Evaluate the model and log its metrics after how\n",
    "    many epochs\n",
    "    :return None\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    val_samples = ceil(n_samples * validation_frac)\n",
    "    trn_samples = n_samples - val_samples\n",
    "    \n",
    "    n_batches = ceil(trn_samples / BATCH_SIZE)\n",
    "    \n",
    "    # first permute before separating the validation samples\n",
    "    permutations = torch.randperm(n_samples)\n",
    "    X = X[permutations]\n",
    "    y = y[permutations]\n",
    "    \n",
    "    X_val = X[:val_samples]\n",
    "    y_val = y[:val_samples]\n",
    "    \n",
    "    # the remaining are train samples\n",
    "    X_trn = X[val_samples:]\n",
    "    y_trn = y[val_samples:]\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # ensuring that the models sees samples in different order / epoch\n",
    "        permutations = torch.randperm(trn_samples)\n",
    "        X_trn = X_trn[permutations]\n",
    "        y_trn = y_trn[permutations]\n",
    "\n",
    "        for batch_no in range(n_batches):            \n",
    "            lower = batch_no * BATCH_SIZE\n",
    "            upper = min(n_samples, (batch_no + 1) * BATCH_SIZE)\n",
    "\n",
    "            x_in = X_trn[lower:upper]\n",
    "            y_in = y_trn[lower:upper]\n",
    "\n",
    "            model.zero_grad()\n",
    "            class_scores = model(x_in)\n",
    "            loss = criterion(class_scores, y_in)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % log_every == 0:\n",
    "            predictions = model(X_trn).argmax(-1)\n",
    "            trn_accuracy = metrics.accuracy_score(predictions, y_trn)\n",
    "\n",
    "            predictions = model(X_val).argmax(-1)\n",
    "            val_accuracy = metrics.accuracy_score(predictions, y_val)\n",
    "            print('Epoch %4d train loss: %4.4f train accuracy %4.4f valid. accuracy %4.4f' % \n",
    "                  (epoch, epoch_loss, trn_accuracy, val_accuracy))\n",
    "\n",
    "\n",
    "# network hyperparams\n",
    "HIDDEN_SIZE = 20\n",
    "N_BITS = 10\n",
    "N_LABELS = 4\n",
    "EPOCHS = 20000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "# building the dataset\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "X, y = build_dataset(101, 1024, N_BITS)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# the model\n",
    "model = FizzBuzz(N_BITS, HIDDEN_SIZE, HIDDEN_SIZE, N_LABELS)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.from_numpy(y)\n",
    "            \n",
    "trainer(model, n_epochs=EPOCHS, batch_size=BATCH_SIZE, X=X, y=y, validation_frac=0.2, log_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accyracy:  0.79\n"
     ]
    }
   ],
   "source": [
    "# test metrics \n",
    "\n",
    "X_tst, y_tst = build_dataset(1, 100, N_BITS)\n",
    "X_tst = torch.tensor(X_tst, dtype=torch.float32)\n",
    "y_tst = encoder.transform(y_tst)\n",
    "\n",
    "model.eval()\n",
    "y_hat = model(X_tst).argmax(-1)\n",
    "\n",
    "acc = metrics.accuracy_score(y_tst, y_hat)\n",
    "print('Test accyracy: ', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
